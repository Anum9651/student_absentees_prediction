# -*- coding: utf-8 -*-
"""student_absentee_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPwcPeU3vKPdOIiraVose3DFWiDBKPtv
"""

# Import required libraries
from google.colab import files
import pandas as pd

# Upload the file
uploaded = files.upload()

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset (replace 'attendance.csv' with your actual file path)
df = pd.read_csv('2018-2019_Daily_Attendance_20240429.csv')

# Feature selection (use 'Enrolled', 'Absent' as features to predict 'Present')
X = df[['Enrolled', 'Absent']]
y = df['Present']

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add a constant term for the intercept in the statsmodels model
X_train_sm = sm.add_constant(X_train)
X_test_sm = sm.add_constant(X_test)  # Add constant term to the test set as well

# Create the linear regression model
model = sm.OLS(y_train, X_train_sm).fit()

# Make predictions
y_pred = model.predict(X_test_sm)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate R-squared
r_squared = r2_score(y_test, y_pred)

# Print results
print(f'R-squared: {r_squared:.4f}')
print(f'Mean Squared Error: {mse:.4f}')

import pandas as pd
import numpy as np
from sklearn.linear_model import RANSACRegressor, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('2018-2019_Daily_Attendance_20240429.csv')

# Drop non-numeric columns
df_numeric = df.drop(['School DBN', 'Date'], axis=1)

# Prepare data
X = df_numeric[['Enrolled', 'Absent']].values  # Independent variables
y = df_numeric['Present'].values                # Dependent variable

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize RANSAC regressor
ransac = RANSACRegressor(random_state=42)  # Use the default LinearRegression as base estimator

# Fit the RANSAC model
ransac.fit(X_train, y_train)

# Make predictions
y_pred = ransac.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'RANSAC Regression Mean Squared Error: {mse:.2f}')
print(f'RANSAC Regression R^2 Score: {r2:.2f}')

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--', label='Ideal Fit')
plt.xlabel('Actual Present')
plt.ylabel('Predicted Present')
plt.title('RANSAC Regression Predictions')
plt.legend()
plt.show()

import pandas as pd

# Load the dataset
df = pd.read_csv('2018-2019_Daily_Attendance_20240429.csv')

# Drop non-numeric columns
df_numeric = df.drop(['School DBN', 'Date'], axis=1)

# Initialize a dictionary to store outlier counts for each column
outlier_counts = {}

# Loop through each column to find outliers
for column in df_numeric.columns:
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    Q1 = df_numeric[column].quantile(0.25)
    Q3 = df_numeric[column].quantile(0.75)

    # Calculate IQR (Interquartile Range)
    IQR = Q3 - Q1

    # Determine outlier boundaries
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Count outliers
    outliers = (df_numeric[column] < lower_bound) | (df_numeric[column] > upper_bound)
    outlier_count = outliers.sum()

    # Store the outlier count in the dictionary
    outlier_counts[column] = outlier_count

# Print out the number of outliers for each column
for column, count in outlier_counts.items():
    print(f'Number of outliers in column "{column}": {count}')

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Convert the problem into a classification task
# Define a threshold for classification (e.g., consider a student 'present' if Present > 90% of Enrolled)
threshold = 0.9
y_train_class = (y_train >= threshold * X_train['Enrolled']).astype(int)  # 1 if present >= 90% enrolled, else 0
y_test_class = (y_test >= threshold * X_test['Enrolled']).astype(int)

# Train a Logistic Regression model for classification purposes
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train, y_train_class)

# Predict on the test set
y_pred_class = clf.predict(X_test)

# Compute accuracy, precision, recall, and F1-score
accuracy = accuracy_score(y_test_class, y_pred_class)
precision = precision_score(y_test_class, y_pred_class)
recall = recall_score(y_test_class, y_pred_class)
f1 = f1_score(y_test_class, y_pred_class)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset (replace 'your_data.csv' with your actual file)
df = pd.read_csv('2018-2019_Daily_Attendance_20240429.csv')

# Drop non-numeric columns (like 'School DBN' and 'Date')
df_numeric = df.drop(['School DBN', 'Date'], axis=1)

# Compute the correlation matrix
correlation_matrix = df_numeric.corr()

# Display the correlation matrix
print(correlation_matrix)

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix Heatmap')
plt.show()